{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Temporal Crop Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "Data\n",
    "- Data augmentation for segmentation\n",
    "\n",
    "- Compute certain features as inputs [Link](https://towardsdatascience.com/satellite-imagery-analysis-using-python-9f389569862c)\n",
    "    - NDVI = (NIR - RED) / (NIR + RED)\n",
    "    - EVI\n",
    "    - RVI = NIR / RED\n",
    "    - NIR + SWIR1 + Blue (Healthy Vegetation Band Combination)\n",
    "\n",
    "Approaches\n",
    "- Same chip different timestamp -> same features (SSL)\n",
    "\n",
    "Model\n",
    "- U-Net\n",
    "- \n",
    "\n",
    "Others\n",
    "- torchgeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torch.distributed as dist\n",
    "import torchmetrics\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('./code'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 14\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import SegmentationTrainTransform, SegmentationValTransform\n",
    "\n",
    "transform_train = SegmentationTrainTransform()\n",
    "transform_val = SegmentationValTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GeoCropDataset(Dataset):\n",
    "    def __init__(self, root='data/', is_train=True, transform=None):\n",
    "        super().__init__()\n",
    "        file_name = 'training_data.txt' if is_train else 'validation_data.txt'\n",
    "        with open(os.path.join(root, file_name)) as f:\n",
    "            chip_list = [line.rstrip() for line in f]\n",
    "        self.img_list = [os.path.join(root, 'hls', chip + '_merged.tif') for chip in chip_list]\n",
    "        self.mask_list = [os.path.join(root, 'masks', chip + '.mask.tif') for chip in chip_list]\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_list[index]\n",
    "        mask_path = self.mask_list[index]\n",
    "        imgs = gdal.Open(img_path, gdal.GA_ReadOnly).ReadAsArray()\n",
    "        mask = gdal.Open(mask_path, gdal.GA_ReadOnly).ReadAsArray()\n",
    "\n",
    "        assert imgs.shape == (18, 224, 224)\n",
    "        assert mask.shape == (224, 224)\n",
    "\n",
    "        if self.transform:\n",
    "            imgs, mask = self.transform(imgs, mask)\n",
    "\n",
    "        return imgs, mask\n",
    "\n",
    "\n",
    "dataset_train = GeoCropDataset(root='data/', is_train=True, transform=transform_train)\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_val = GeoCropDataset(root='data/', is_train=False, transform=transform_val)\n",
    "loader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "imgs, mask = next(iter(loader_train))\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 14, 224, 224]         --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-1            [1, 64, 224, 224]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 224, 224]         10,432\n",
       "│    │    └─ReLU: 3-2                    [1, 64, 224, 224]         --\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 224, 224]         36,928\n",
       "│    │    └─ReLU: 3-4                    [1, 64, 224, 224]         --\n",
       "├─ModuleList: 1-8                        --                        --\n",
       "│    └─DownSample: 2-2                   [1, 64, 112, 112]         --\n",
       "│    │    └─MaxPool2d: 3-5               [1, 64, 112, 112]         --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-3            [1, 128, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-6                  [1, 128, 112, 112]        73,856\n",
       "│    │    └─ReLU: 3-7                    [1, 128, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-8                  [1, 128, 112, 112]        147,584\n",
       "│    │    └─ReLU: 3-9                    [1, 128, 112, 112]        --\n",
       "├─ModuleList: 1-8                        --                        --\n",
       "│    └─DownSample: 2-4                   [1, 128, 56, 56]          --\n",
       "│    │    └─MaxPool2d: 3-10              [1, 128, 56, 56]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-5            [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-11                 [1, 256, 56, 56]          295,168\n",
       "│    │    └─ReLU: 3-12                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 256, 56, 56]          590,080\n",
       "│    │    └─ReLU: 3-14                   [1, 256, 56, 56]          --\n",
       "├─ModuleList: 1-8                        --                        --\n",
       "│    └─DownSample: 2-6                   [1, 256, 28, 28]          --\n",
       "│    │    └─MaxPool2d: 3-15              [1, 256, 28, 28]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-7            [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 512, 28, 28]          1,180,160\n",
       "│    │    └─ReLU: 3-17                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-18                 [1, 512, 28, 28]          2,359,808\n",
       "│    │    └─ReLU: 3-19                   [1, 512, 28, 28]          --\n",
       "├─ModuleList: 1-8                        --                        --\n",
       "│    └─DownSample: 2-8                   [1, 512, 14, 14]          --\n",
       "│    │    └─MaxPool2d: 3-20              [1, 512, 14, 14]          --\n",
       "├─DoubleConvolution: 1-9                 [1, 1024, 14, 14]         --\n",
       "│    └─Conv2d: 2-9                       [1, 1024, 14, 14]         4,719,616\n",
       "│    └─ReLU: 2-10                        [1, 1024, 14, 14]         --\n",
       "│    └─Conv2d: 2-11                      [1, 1024, 14, 14]         9,438,208\n",
       "│    └─ReLU: 2-12                        [1, 1024, 14, 14]         --\n",
       "├─ModuleList: 1-19                       --                        (recursive)\n",
       "│    └─UpSample: 2-13                    [1, 512, 28, 28]          --\n",
       "│    │    └─ConvTranspose2d: 3-21        [1, 512, 28, 28]          2,097,664\n",
       "├─ModuleList: 1-20                       --                        --\n",
       "│    └─CropAndConcat: 2-14               [1, 1024, 28, 28]         --\n",
       "├─ModuleList: 1-21                       --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-15           [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 512, 28, 28]          4,719,104\n",
       "│    │    └─ReLU: 3-23                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-24                 [1, 512, 28, 28]          2,359,808\n",
       "│    │    └─ReLU: 3-25                   [1, 512, 28, 28]          --\n",
       "├─ModuleList: 1-19                       --                        (recursive)\n",
       "│    └─UpSample: 2-16                    [1, 256, 56, 56]          --\n",
       "│    │    └─ConvTranspose2d: 3-26        [1, 256, 56, 56]          524,544\n",
       "├─ModuleList: 1-20                       --                        --\n",
       "│    └─CropAndConcat: 2-17               [1, 512, 56, 56]          --\n",
       "├─ModuleList: 1-21                       --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-18           [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-27                 [1, 256, 56, 56]          1,179,904\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 56, 56]          590,080\n",
       "│    │    └─ReLU: 3-30                   [1, 256, 56, 56]          --\n",
       "├─ModuleList: 1-19                       --                        (recursive)\n",
       "│    └─UpSample: 2-19                    [1, 128, 112, 112]        --\n",
       "│    │    └─ConvTranspose2d: 3-31        [1, 128, 112, 112]        131,200\n",
       "├─ModuleList: 1-20                       --                        --\n",
       "│    └─CropAndConcat: 2-20               [1, 256, 112, 112]        --\n",
       "├─ModuleList: 1-21                       --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-21           [1, 128, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 112, 112]        295,040\n",
       "│    │    └─ReLU: 3-33                   [1, 128, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-34                 [1, 128, 112, 112]        147,584\n",
       "│    │    └─ReLU: 3-35                   [1, 128, 112, 112]        --\n",
       "├─ModuleList: 1-19                       --                        (recursive)\n",
       "│    └─UpSample: 2-22                    [1, 64, 224, 224]         --\n",
       "│    │    └─ConvTranspose2d: 3-36        [1, 64, 224, 224]         32,832\n",
       "├─ModuleList: 1-20                       --                        --\n",
       "│    └─CropAndConcat: 2-23               [1, 128, 224, 224]        --\n",
       "├─ModuleList: 1-21                       --                        (recursive)\n",
       "│    └─DoubleConvolution: 2-24           [1, 64, 224, 224]         --\n",
       "│    │    └─Conv2d: 3-37                 [1, 64, 224, 224]         73,792\n",
       "│    │    └─ReLU: 3-38                   [1, 64, 224, 224]         --\n",
       "│    │    └─Conv2d: 3-39                 [1, 64, 224, 224]         36,928\n",
       "│    │    └─ReLU: 3-40                   [1, 64, 224, 224]         --\n",
       "├─Conv2d: 1-22                           [1, 14, 224, 224]         910\n",
       "==========================================================================================\n",
       "Total params: 31,041,230\n",
       "Trainable params: 31,041,230\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 42.32\n",
       "==========================================================================================\n",
       "Input size (MB): 3.61\n",
       "Forward/backward pass size (MB): 249.68\n",
       "Params size (MB): 124.16\n",
       "Estimated Total Size (MB): 377.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DoubleConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Two 3x3 Convolution Layers.\n",
    "    In the U-Net paper they used $0$ padding, but we use $1$ padding so that final feature map is not cropped.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.first(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.second(x)\n",
    "        return self.act2(x)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down-sample\n",
    "\n",
    "    Each step in the contracting path down-samples the feature map with\n",
    "    a $2 \\times 2$ max pooling layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up-sample\n",
    "\n",
    "    Each step in the expansive path up-samples the feature map with\n",
    "    a $2 \\times 2$ up-convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Up-convolution\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class CropAndConcat(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Crop and Concatenate the feature map\n",
    "\n",
    "    At every step in the expansive path the corresponding feature map from the contracting path\n",
    "    concatenated with the current feature map.\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: current feature map in the expansive path\n",
    "        :param contracting_x: corresponding feature map from the contracting path\n",
    "        \"\"\"\n",
    "\n",
    "        # Crop the feature map from the contracting path to the size of the current feature map\n",
    "        contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n",
    "        # Concatenate the feature maps\n",
    "        x = torch.cat([x, contracting_x], dim=1)\n",
    "        #\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=18, out_channels=14):\n",
    "        \"\"\"\n",
    "        :param in_channels: number of channels in the input image\n",
    "        :param out_channels: number of channels in the result feature map\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Double convolution layers for the contracting path.\n",
    "        # The number of features gets doubled at each step starting from $64$.\n",
    "        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n",
    "                                        [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n",
    "        # Down sampling layers for the contracting path\n",
    "        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n",
    "\n",
    "        # The two convolution layers at the lowest resolution (the bottom of the U).\n",
    "        self.middle_conv = DoubleConvolution(512, 1024)\n",
    "\n",
    "        # Up sampling layers for the expansive path.\n",
    "        # The number of features is halved with up-sampling.\n",
    "        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in\n",
    "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
    "        # Double convolution layers for the expansive path.\n",
    "        # Their input is the concatenation of the current feature map and the feature map from the\n",
    "        # contracting path. Therefore, the number of input features is double the number of features\n",
    "        # from up-sampling.\n",
    "        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n",
    "                                      [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
    "        # Crop and concatenate layers for the expansive path.\n",
    "        self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: input image\n",
    "        \"\"\"\n",
    "        # To collect the outputs of contracting path for later concatenation with the expansive path.\n",
    "        pass_through = []\n",
    "        # Contracting path\n",
    "        for i in range(len(self.down_conv)):\n",
    "            x = self.down_conv[i](x)\n",
    "            pass_through.append(x)\n",
    "            x = self.down_sample[i](x)\n",
    "\n",
    "        x = self.middle_conv(x)\n",
    "\n",
    "        # Expansive path\n",
    "        for i in range(len(self.up_conv)):\n",
    "            x = self.up_sample[i](x)\n",
    "            x = self.concat[i](x, pass_through.pop())\n",
    "            x = self.up_conv[i](x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = UNet(in_channels=18, out_channels=n_classes)\n",
    "model(imgs.float()).shape\n",
    "\n",
    "summary(model, input_size=(1, 18, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
    "    Args:\n",
    "        in_channels (int)-- number of input features.\n",
    "        out_channels (int) -- number of output features.\n",
    "        kernel_size (int) -- Size of convolution kernel.\n",
    "        stride (int) -- decides how jumpy kernel moves along the spatial dimensions.\n",
    "        padding (int) -- how much the input should be padded on the borders with zero.\n",
    "        dilation (int) -- dilation ratio for enlarging the receptive field.\n",
    "        num_conv_layers (int) -- Number of conv+BN+ReLU layers in the block.\n",
    "        drop_rate (float) -- dropout rate at the end of the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                            stride=stride, padding=padding, dilation=dilation, bias=False),\n",
    "                  nn.BatchNorm2d(out_channels),\n",
    "                  nn.ReLU(inplace=True), ]\n",
    "\n",
    "        if num_conv_layers > 1:\n",
    "            if drop_rate > 0:\n",
    "                layers += [nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
    "                                     stride=stride, padding=padding, dilation=dilation, bias=False),\n",
    "                           nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "                           nn.Dropout(drop_rate), ] * (num_conv_layers - 1)\n",
    "            else:\n",
    "                layers += [nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                     padding=padding, dilation=dilation, bias=False),\n",
    "                           nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), ] * (num_conv_layers - 1)\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.block(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class DUC(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense Upscaling Convolution (DUC) layer.\n",
    "        \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        upscale (int): Upscaling factor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor after applying DUC.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channles, upscale):\n",
    "        super(DUC, self).__init__()\n",
    "        out_channles = out_channles * (upscale ** 2)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channles, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channles)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pixl_shf = nn.PixelShuffle(upscale_factor=upscale)\n",
    "\n",
    "        kernel = self.icnr(self.conv.weight, scale=upscale)\n",
    "        self.conv.weight.data.copy_(kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = self.pixl_shf(x)\n",
    "        return x\n",
    "\n",
    "    def icnr(self, x, scale=2, init=nn.init.kaiming_normal):\n",
    "        \"\"\"\n",
    "        ICNR (Initialization from Corresponding Normalized Response) function.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            scale (int): Upscaling factor.\n",
    "            init (function): Initialization function.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Initialized kernel.\n",
    "        Note:\n",
    "            Even with pixel shuffle we still have check board artifacts,\n",
    "            the solution is to initialize the d**2 feature maps with the same\n",
    "            radom weights: https://arxiv.org/pdf/1707.02937.pdf\n",
    "        \"\"\"\n",
    "\n",
    "        new_shape = [int(x.shape[0] / (scale ** 2))] + list(x.shape[1:])\n",
    "        subkernel = torch.zeros(new_shape)\n",
    "        subkernel = init(subkernel)\n",
    "        subkernel = subkernel.transpose(0, 1)\n",
    "        subkernel = subkernel.contiguous().view(subkernel.shape[0],\n",
    "                                                subkernel.shape[1], -1)\n",
    "        kernel = subkernel.repeat(1, 1, scale ** 2)\n",
    "        transposed_shape = [x.shape[1]] + [x.shape[0]] + list(x.shape[2:])\n",
    "        kernel = kernel.contiguous().view(transposed_shape)\n",
    "        kernel = kernel.transpose(0, 1)\n",
    "        return kernel\n",
    "\n",
    "\n",
    "class UpconvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder layer decodes the features along the expansive path.\n",
    "    Args:\n",
    "        in_channels (int) -- number of input features.\n",
    "        out_channels (int) -- number of output features.\n",
    "        upmode (str) -- Upsampling type. If \"fixed\" then a linear upsampling with scale factor\n",
    "                        of two will be applied using bi-linear as interpolation method.\n",
    "                        If deconv_1 is chosen then a non-overlapping transposed convolution will\n",
    "                        be applied to upsample the feature maps. If deconv_1 is chosen then an\n",
    "                        overlapping transposed convolution will be applied to upsample the feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, upmode=\"deconv_1\"):\n",
    "        super(UpconvBlock, self).__init__()\n",
    "\n",
    "        if upmode == \"fixed\":\n",
    "            layers = [nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True), ]\n",
    "            layers += [nn.BatchNorm2d(in_channels),\n",
    "                       nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False), ]\n",
    "\n",
    "        elif upmode == \"deconv_1\":\n",
    "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, dilation=1), ]\n",
    "\n",
    "        elif upmode == \"deconv_2\":\n",
    "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, dilation=1), ]\n",
    "\n",
    "        # Dense Upscaling Convolution\n",
    "        elif upmode == \"DUC\":\n",
    "            up_factor = 2\n",
    "            upsample_dim = (up_factor ** 2) * out_channels\n",
    "            layers = [nn.Conv2d(in_channels, upsample_dim, kernel_size=3, padding=1),\n",
    "                      nn.BatchNorm2d(upsample_dim),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.PixelShuffle(up_factor), ]\n",
    "            \n",
    "            #layers = [DUC(in_channels, out_channels, upscale=2)]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Provided upsampling mode is not recognized.\")\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.block(inputs)\n",
    "\n",
    "\n",
    "class AdditiveAttentionBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    additive attention gate (AG) to merge feature maps extracted at multiple scales through skip connection.\n",
    "\n",
    "    Args:\n",
    "        f_g (int) -- number of feature maps collected from the higher resolution in encoder path.\n",
    "        f_x (int) -- number of feature maps in layer \"x\" in the decoder.\n",
    "        f_inter (int) -- number of feature maps after summation equal to the number of\n",
    "                       learnable multidimensional attention coefficients.\n",
    "\n",
    "    Note: Unlike the original paper we upsample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_x, F_inter):\n",
    "        super(AdditiveAttentionBlock, self).__init__()\n",
    "\n",
    "        # Decoder\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_inter, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_inter)\n",
    "        )\n",
    "        # Encoder\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_x, F_inter, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_inter)\n",
    "        )\n",
    "\n",
    "        # Fused\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_inter, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # set_trace()\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        merge = self.relu(g1 + x1)\n",
    "        psi = self.psi(merge)\n",
    "\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels, filter_config=None, use_skipAtt=False, dropout_rate=0):\n",
    "        \"\"\"\n",
    "        UNet model with optional additive attention between skip connections \n",
    "        for semantic segmentation of multispectral satellite images.\n",
    "\n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            in_channels (int): Number of input channels.\n",
    "            filter_config (tuple, optional): Configuration of filters in the contracting path.\n",
    "                        Default is None, which uses the configuration (64, 128, 256, 512, 1024, 2048).\n",
    "            use_skipAtt (bool, optional): Flag indicating whether to use skip connections with attention.\n",
    "                        Default is False.\n",
    "            dropout_rate (float, optional): Dropout rate applied to the convolutional layers.\n",
    "                        Default is 0.\n",
    "\n",
    "        \"\"\"\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.use_skipAtt = use_skipAtt\n",
    "\n",
    "        if not filter_config:\n",
    "            filter_config = (64, 128, 256, 512, 1024, 2048)\n",
    "\n",
    "        assert len(filter_config) == 6\n",
    "\n",
    "        # Contraction Path\n",
    "        self.encoder_1 = ConvBlock(self.in_channels, filter_config[0], num_conv_layers=2,\n",
    "                                   drop_rate=dropout_rate)  # 64x224x224\n",
    "        self.encoder_2 = ConvBlock(filter_config[0], filter_config[1], num_conv_layers=2,\n",
    "                                   drop_rate=dropout_rate)  # 128x112x112\n",
    "        self.encoder_3 = ConvBlock(filter_config[1], filter_config[2], num_conv_layers=2,\n",
    "                                   drop_rate=dropout_rate)  # 256x56x56\n",
    "        self.encoder_4 = ConvBlock(filter_config[2], filter_config[3], num_conv_layers=2,\n",
    "                                   drop_rate=dropout_rate)  # 512x28x28\n",
    "        self.encoder_5 = ConvBlock(filter_config[3], filter_config[4], num_conv_layers=2,\n",
    "                                   drop_rate=dropout_rate)  # 1024x14x14\n",
    "        self.encoder_6 = ConvBlock(filter_config[4], filter_config[5], num_conv_layers=2,\n",
    "                                   drop_rate=dropout_rate)  # 2048x7x7\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Expansion Path\n",
    "        self.decoder_1 = UpconvBlock(filter_config[5], filter_config[4], upmode=\"deconv_2\")  # 1024x14x14\n",
    "        self.conv1 = ConvBlock(filter_config[4] * 2, filter_config[4], num_conv_layers=2, drop_rate=dropout_rate)\n",
    "\n",
    "        self.decoder_2 = UpconvBlock(filter_config[4], filter_config[3], upmode=\"deconv_2\")  # 512x28x28\n",
    "        self.conv2 = ConvBlock(filter_config[3] * 2, filter_config[3], num_conv_layers=2, drop_rate=dropout_rate)\n",
    "\n",
    "        self.decoder_3 = UpconvBlock(filter_config[3], filter_config[2], upmode=\"deconv_2\")  # 256x56x56\n",
    "        self.conv3 = ConvBlock(filter_config[2] * 2, filter_config[2], num_conv_layers=2, drop_rate=dropout_rate)\n",
    "\n",
    "        self.decoder_4 = UpconvBlock(filter_config[2], filter_config[1], upmode=\"deconv_2\")  # 128x112x112\n",
    "        self.conv4 = ConvBlock(filter_config[1] * 2, filter_config[1], num_conv_layers=2, drop_rate=dropout_rate)\n",
    "\n",
    "        self.decoder_5 = UpconvBlock(filter_config[1], filter_config[0], upmode=\"deconv_2\")  # 64x224x224\n",
    "        self.conv5 = ConvBlock(filter_config[0] * 2, filter_config[0], num_conv_layers=2, drop_rate=dropout_rate)\n",
    "\n",
    "        if self.use_skipAtt:\n",
    "            self.Att1 = AdditiveAttentionBlock(F_g=filter_config[4], F_x=filter_config[4], F_inter=filter_config[3])\n",
    "            self.Att2 = AdditiveAttentionBlock(F_g=filter_config[3], F_x=filter_config[3], F_inter=filter_config[2])\n",
    "            self.Att3 = AdditiveAttentionBlock(F_g=filter_config[2], F_x=filter_config[2], F_inter=filter_config[1])\n",
    "            self.Att4 = AdditiveAttentionBlock(F_g=filter_config[1], F_x=filter_config[1], F_inter=filter_config[0])\n",
    "            self.Att5 = AdditiveAttentionBlock(F_g=filter_config[0], F_x=filter_config[0],\n",
    "                                               F_inter=int(filter_config[0] / 2))\n",
    "\n",
    "        self.classifier = nn.Conv2d(filter_config[0], n_classes, kernel_size=1, stride=1, padding=0)  # classNumx224x224\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the UNet model.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, n_classes, height, width).\n",
    "\n",
    "        \"\"\"\n",
    "        e1 = self.encoder_1(inputs)  # batch size x 64 x 224 x 224\n",
    "        p1 = self.pool(e1)  # batch size x 64 x 112 x 112\n",
    "\n",
    "        e2 = self.encoder_2(p1)  # batch size x 128 x 112 x 112\n",
    "        p2 = self.pool(e2)  # batch size x 128 x 56 x 56\n",
    "\n",
    "        e3 = self.encoder_3(p2)  # batch size x 256 x 56 x 56\n",
    "        p3 = self.pool(e3)  # batch size x 256 x 28 x 28\n",
    "\n",
    "        e4 = self.encoder_4(p3)  # batch size x 512 x 28 x 28\n",
    "        p4 = self.pool(e4)  # batch size x 1024 x 14 x 14\n",
    "\n",
    "        e5 = self.encoder_5(p4)  # batch size x 1024 x 14 x 14\n",
    "        p5 = self.pool(e5)  # batch size x 1024 x 7 x 7\n",
    "\n",
    "        e6 = self.encoder_6(p5)  # batch size x 2048 x 7 x 7\n",
    "\n",
    "        d6 = self.decoder_1(e6)  # batch size x 1024 x 14 x 14\n",
    "\n",
    "        if self.use_skipAtt:\n",
    "            x5 = self.Att1(g=d6, x=e5)  # batch size x 1024 x 14 x 14\n",
    "            skip1 = torch.cat((x5, d6), dim=1)  # batch size x 2048 x 14 x 14\n",
    "        else:\n",
    "            skip1 = torch.cat((e5, d6), dim=1)  # batch size x 2048 x 14 x 14\n",
    "\n",
    "        d6_proper = self.conv1(skip1)  # batch size x 1024 x 14 x 14\n",
    "\n",
    "        d5 = self.decoder_2(d6_proper)  # batch size x 512 x 28 x 28\n",
    "\n",
    "        if self.use_skipAtt:\n",
    "            x4 = self.Att2(g=d5, x=e4)  # batch size x 512 x 28 x 28\n",
    "            skip2 = torch.cat((x4, d5), dim=1)  # batch size x 1024 x 28 x 28\n",
    "        else:\n",
    "            skip2 = torch.cat((e4, d5), dim=1)  # batch size x 1024 x 28 x 28\n",
    "\n",
    "        d5_proper = self.conv2(skip2)  # batch size x 512 x 28 x 28\n",
    "\n",
    "        d4 = self.decoder_3(d5_proper)  # batch size x 256 x 56 x 56\n",
    "\n",
    "        if self.use_skipAtt:\n",
    "            x3 = self.Att3(g=d4, x=e3)  # batch size x 256 x 56 x 56\n",
    "            skip3 = torch.cat((x3, d4), dim=1)  # batch size x 512 x 56 x 56\n",
    "        else:\n",
    "            skip3 = torch.cat((e3, d4), dim=1)  # batch size x 512 x 56 x 56\n",
    "\n",
    "        d4_proper = self.conv3(skip3)  # batch size x 256 x 56 x 56\n",
    "\n",
    "        d3 = self.decoder_4(d4_proper)  # batch size x 128 x 112 x 112\n",
    "\n",
    "        if self.use_skipAtt:\n",
    "            x2 = self.Att4(g=d3, x=e2)  # batch size x 128 x 112 x 112\n",
    "            skip4 = torch.cat((x2, d3), dim=1)  # batch size x 256 x 112 x 112\n",
    "        else:\n",
    "            skip4 = torch.cat((e2, d3), dim=1)  # batch size x 256 x 112 x 112\n",
    "\n",
    "        d3_proper = self.conv4(skip4)  # batch size x 128 x 112 x 112\n",
    "\n",
    "        d2 = self.decoder_5(d3_proper)  # batch size x 64 x 224 x 224\n",
    "\n",
    "        if self.use_skipAtt:\n",
    "            x1 = self.Att5(g=d2, x=e1)  # batch size x 64 x 224 x 224\n",
    "            skip5 = torch.cat((x1, d2), dim=1)  # batch size x 128 x 224 x 224\n",
    "        else:\n",
    "            skip5 = torch.cat((e1, d2), dim=1)  # batch size x 128 x 224 x 224\n",
    "\n",
    "        d2_proper = self.conv5(skip5)  # batch size x 64 x 224 x 224\n",
    "\n",
    "        d1 = self.classifier(d2_proper)  # batch size x classNum x 224 x 224\n",
    "\n",
    "        return d1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 96\u001b[0m\n\u001b[1;32m     90\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: loader_train, \n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: loader_val\n\u001b[1;32m     93\u001b[0m }\n\u001b[1;32m     94\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 96\u001b[0m losses, jaccard_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, dataloaders, num_epochs, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     52\u001b[0m     out \u001b[38;5;241m=\u001b[39m outputs\n\u001b[0;32m---> 53\u001b[0m     masks_batch \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m     masks_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([masks_pred, masks_batch], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     55\u001b[0m     masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([masks, targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(\n",
    "        model, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        dataloaders, \n",
    "        num_epochs, \n",
    "        device = 'cpu',\n",
    "):\n",
    "    t_start = time.time()\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    best_model_weights = copy.deepcopy(model_without_ddp.state_dict())\n",
    "\n",
    "    best_jac = 0.0\n",
    "    losses = {'train': [], 'val': []}\n",
    "    jaccard_indices = {'train': [], 'val': []}\n",
    "    jaccard = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=14)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t1 = time.time()\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_indices = 0.0\n",
    "            masks_pred = np.zeros((0, 224, 224))\n",
    "            masks = np.zeros((0, 224, 224))\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = criterion(outputs, targets.long())\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    out = outputs\n",
    "                    masks_batch = out.cpu().detach().numpy().argmax(1)\n",
    "                    masks_pred = np.concatenate([masks_pred, masks_batch], axis=0)\n",
    "                    masks = np.concatenate([masks, targets.cpu().detach().numpy()], axis=0)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            jac = jaccard(torch.Tensor(masks_pred), torch.Tensor(masks))\n",
    "\n",
    "            loss_avg = epoch_loss\n",
    "            jac_avg = jac\n",
    "\n",
    "            losses[phase].append(loss_avg)\n",
    "            jaccard_indices[phase].append(jac_avg)\n",
    "\n",
    "            if phase == 'train':\n",
    "                print(f'Epoch [{epoch+1:4d}/{num_epochs:4d}]   Train Loss: {loss_avg:.3e}, Jaccard: {jac_avg:.4f}', end='')\n",
    "            else:\n",
    "                print(f'   Val Loss: {loss_avg:.3e}, Jaccard: {jac_avg:.4f}   Time: {(time.time()-t1):.0f}s')\n",
    "\n",
    "            # save best model\n",
    "            if phase == 'val' and jac_avg > best_jac:\n",
    "                best_jac = jac_avg\n",
    "                best_model_weights = copy.deepcopy(model_without_ddp.state_dict())\n",
    "                torch.save(best_model_weights, './checkpoints/model_unet_best.pth')\n",
    "\n",
    "    time_elapsed = time.time() - t_start\n",
    "    print(f'Training completed in {time_elapsed // 60:.0f} min {time_elapsed % 60:.0f} s')\n",
    "    print(f'Best Validation Jaccard: {best_jac:.4f}')\n",
    "\n",
    "    return losses, jaccard_indices\n",
    "\n",
    "\n",
    "\n",
    "model = UNet(in_channels=18, out_channels=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "dataloaders = {\n",
    "    \"train\": loader_train, \n",
    "    \"val\": loader_val\n",
    "}\n",
    "num_epochs = 3\n",
    "\n",
    "losses, jaccard_indices = train_model(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    dataloaders,\n",
    "    num_epochs,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis/Files/deep-learning/cv-image-segmentation-forecast/code/utils.py:71: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
      "/Users/louis/Files/deep-learning/cv-image-segmentation-forecast/code/utils.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  tp / (tp + fp + fn),\n",
      "/Users/louis/Files/deep-learning/cv-image-segmentation-forecast/code/utils.py:82: RuntimeWarning: invalid value encountered in divide\n",
      "  tp / (tp + fp),\n",
      "/Users/louis/Files/deep-learning/cv-image-segmentation-forecast/code/utils.py:95: RuntimeWarning: invalid value encountered in divide\n",
      "  tp / (tp + fn),\n",
      "/Users/louis/Files/deep-learning/cv-image-segmentation-forecast/code/utils.py:109: RuntimeWarning: invalid value encountered in divide\n",
      "  2 * (precision * recall) / (precision + recall),\n"
     ]
    }
   ],
   "source": [
    "from utils import do_accuracy_evaluation, Evaluator\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load('./checkpoints/model_unet_best.pth'))\n",
    "model.to(device)\n",
    "\n",
    "class_mapping = {\n",
    "  \"0\": \"Unknown\",\n",
    "  \"1\": \"Natural Vegetation\",\n",
    "  \"2\": \"Forest\",\n",
    "  \"3\": \"Corn\",\n",
    "  \"4\": \"Soybeans\",\n",
    "  \"5\": \"Wetlands\",\n",
    "  \"6\": \"Developed/Barren\",\n",
    "  \"7\": \"Open Water\",\n",
    "  \"8\": \"Winter Wheat\",\n",
    "  \"9\": \"Alfalfa\",\n",
    "  \"10\": \"Fallow/Idle Cropland\",\n",
    "  \"11\": \"Cotton\",\n",
    "  \"12\": \"Sorghum\",\n",
    "  \"13\": \"Other\"\n",
    "}\n",
    "\n",
    "# do_accuracy_evaluation(model, loader_val, n_classes, class_mapping, device)\n",
    "\n",
    "evaluator = Evaluator(n_classes)\n",
    "\n",
    "model.eval()\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "  for data in loader_val:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    if torch.isnan(outputs).any():\n",
    "        print(\"NaN value found in model outputs!\")\n",
    "    outputs = F.softmax(outputs, 1)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            # add batch to evaluator\n",
    "    evaluator.add_batch(labels.cpu().numpy(), \n",
    "                                preds.cpu().numpy())\n",
    "\n",
    "    # calculate evaluation metrics\n",
    "    overall_accuracy = evaluator.overall_accuracy()\n",
    "    classwise_overal_accuracy = evaluator.classwise_overal_accuracy()\n",
    "    mean_accuracy = np.nanmean(classwise_overal_accuracy)\n",
    "    IoU = evaluator.intersection_over_union()\n",
    "    mean_IoU = np.nanmean(IoU)\n",
    "    precision = evaluator.precision()\n",
    "    mean_precision = np.nanmean(precision)\n",
    "    recall = evaluator.recall()\n",
    "    mean_recall = np.nanmean(recall)\n",
    "    f1_score = evaluator.f1_score()\n",
    "    mean_f1_score = np.nanmean(f1_score)\n",
    "\n",
    "    metrics = {\n",
    "        \"Overall Accuracy\": overall_accuracy,\n",
    "        \"Mean Accuracy\": mean_accuracy,\n",
    "        \"Mean IoU\": mean_IoU,\n",
    "        \"mean Precision\": mean_precision,\n",
    "        \"mean Recall\": mean_recall,\n",
    "        \"Mean F1 Score\": mean_f1_score\n",
    "    }\n",
    "\n",
    "    # print confusion matrix\n",
    "    # evaluator.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall Accuracy': 0.5262502967505095,\n",
       " 'Mean Accuracy': 0.47177132954687007,\n",
       " 'Mean IoU': 0.3000807281347857,\n",
       " 'mean Precision': 0.4651827487605651,\n",
       " 'mean Recall': 0.4380733774363793,\n",
       " 'Mean F1 Score': 0.4388545105428932}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
